<!DOCTYPE html>
<html>
<head>
  <title>WebRTC/PeerJS Audio Call</title>
  <style>
    body{
      /* background-color: beige; */
    }
    video {
      width: 300px;
      object-fit: contain;
    }
    
    div#photobooth {
      position: relative;
      background-image: url('original.jpg');
      background-size: contain;
      height: 640px;
      width: 640px;
      /* right: 0; */
    }
    div#photobooth #lab0{
      position: absolute;
      top: 0;
      left: 0;
    }
    div#photobooth #lab1{
      position:absolute;
      top: 0;
      left: 200px;
    }
  </style>  
  <link rel="stylesheet" href="basic.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/peerjs/1.0.4/peerjs.min.js"></script>
  <script src="./call-peer.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.2"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix@2.0"></script>
  <script src='./bodypix.js'></script>

</head>
<body>
  <div class="wrapper">
    <input type="text" placeholder="Peer ID" id="id">
    <button id="call" onclick="callPeer(document.getElementById('id').value)">Call Peer</button>
    <br/>
    <br/>
    
    <input type="text" placeholder="Send message" id="message">
    <button id="message" onclick="sendMessage(document.getElementById('message').value)">Send Message</button>
    
    <div id="videos"></div>

    <div id="display"></div>
  </div>
  <div id="maskGroup">
      <!-- <canvas id='maskdraw' style='display:none' ></canvas> -->
  </div>
  <div id="photobooth">
    
      
          <!-- <canvas id='lab'></canvas> -->
  </div>
  <script>
    // Create an <audio> element to play the audio stream
    // Create an <video> element to play the video stream
    let videoCount = 0;
    // function renderImageDataToOffScreenCanvas(imageData) {
    //   const canvas = document.getElementById('maskdraw');
    //   canvas.width = imageData.width;
    //   canvas.height = imageData.height;
    //   canvas.getContext('2d').putImageData(imageData, 0, 0);
    // }
    function streamCanvas(video, c){
      let canvasElt = video
      let canvasStream = canvasElt.captureStream(25);
        // Handle incoming video
      if(canvasStream.getVideoTracks().length) {
        console.log("Initialize canvas video track...")
        let canvasVideo = document.createElement('video')
        canvasVideo.autoplay = true
          
        document.getElementById('lab').appendChild(canvasVideo)
        canvasVideo.srcObject = canvasStream
        canvasVideo.setAttribute('id', `canvasVideo${c}`)
        canvasVideo.width = video.videoWidth
        canvasVideo.height = video.videoHeight
        loadAndPredict(canvasVideo)
      } 
    }
    async function loadAndPredict(v, c) {
      const net = await bodyPix.load(/** optional arguments, see below **/);
      console.log(net)
      /**
       * One of (see documentation below):
       *   - net.segmentPerson
       *   - net.segmentPersonParts
       *   - net.segmentMultiPerson
       *   - net.segmentMultiPersonParts
       * See documentation below for details on each method.
       */
      async function estimateSegmentationAndDraw() {
        // console.log('video:', v);
        const segmentation = await net.segmentPerson(v);
        
        const maskBackground = true;
        // Convert the segmentation into a mask to darken the background.
        const backgroundColor = {r: 0, g: 0, b: 0, a: 0};
        const foregroundColor = {r: 255, g: 255, b: 0, a: 255};
        const mask = bodyPix.toMask(
            segmentation, foregroundColor, backgroundColor);
        const opacity = 1;
        const maskBlurAmount = 3;
        const flipHorizontal = true;
        const canvas = document.getElementById(`lab${c}`)
        // console.log(c)
        // Draw the mask onto the image on a canvas.  With opacity set to 0.7 and
        // maskBlurAmount set to 3, this will darken the background and blur the
        // darkened background's edge.
        const ctx = canvas.getContext('2d');
        ctx.restore();
        canvas.width = v.videoWidth;
        canvas.height = v.videoHeight;
        ctx.translate(canvas.width, 0);
        ctx.scale(-1, 1);
        ctx.drawImage(v, 0, 0);
        //ctx.drawImage(mask, 0, 0);
        const offScreenCanvas = document.getElementById(`maskdraw${c}`);
        offScreenCanvas.width = mask.width;
        offScreenCanvas.height = mask.height;
        // console.log(mask.width, mask.height)
        offScreenCanvas.getContext('2d').putImageData(mask, 0, 0);
        ctx.globalCompositeOperation = 'destination-in';
        ctx.drawImage(offScreenCanvas, 0, 0);
        // bodyPix.drawMask(
        //     canvas, v, mask, opacity, maskBlurAmount, flipHorizontal);
        requestAnimationFrame(estimateSegmentationAndDraw);
      }
      estimateSegmentationAndDraw();
    }
    //create new canvas and mask canvas for new video
    function createCanvases(){
      document.getElementById('maskGroup').innerHTML += `<canvas id='maskdraw${videoCount}' style='display:none' ></canvas>`
      document.getElementById('photobooth').innerHTML += `<canvas id='lab${videoCount}'></canvas>`
      
    }
    async function playStream(stream) {
      // Handle incoming audio
      if(stream.getAudioTracks().length) {
        console.log("Initialize audio track...")
        var audio = document.createElement('audio')
        audio.autoplay = true
        
        document.body.appendChild(audio)
        audio.srcObject = stream
      } 
      // Handle incoming video
      if(stream.getVideoTracks().length) {
        console.log("Initialize video track...")
        var video = document.createElement('video')
        video.autoplay = true
        
        document.getElementById('videos').appendChild(video)
        video.srcObject = stream
        video.setAttribute("class", "video");
        video.setAttribute('id', `video${videoCount}`);
        video.setAttribute('style', 'display:none')
        createCanvases()
        const count = videoCount
        video.onloadedmetadata = function() {
          // streamCanvas(video, videoCount)
          video.width = video.videoWidth
          video.height = video.videoHeight
          loadAndPredict(video, count)
          console.log(`${video.id} is detected`)
        }
      } 
      videoCount ++
    }
    
    // initialize Peer.js real-time connection
    // initPeer(
      // connectionCallback: fn(id: string),
      // dataCallback: fn(message: any, peerId: string),
      // mediaCallback: fn(stream: MediaStream, peerId: string),
      // settings: {id: string, audio: bool, video: bool }
    // )
    // initialize Peer.js real-time connection
    initPeer(
      // called on initial connection (on Peer.js initialization)
      function onConnection(myPeerId) {
        console.log("MY ID", myPeerId)
      },
      // called on incoming data messages (every message)
      function onData(message, peerId) {
        console.log("ON DATA", message, peerId)
      },
      // called on incoming media stream connections (on connection)
      // for local media stream, peerId is `undefined`
      function onMediaStream(stream, peerId) {
        // stream is an instance of the MediaStream class
        // it contains both audio and video streaming tracks
        // Here's the documentation: https://developer.mozilla.org/en-US/docs/Web/API/MediaStream
        
        if(peerId) {
          console.log("ON REMOTE STREAM", peerId, stream)
        } else {
          console.log("ON LOCAL STREAM", stream)
        }
        // Play the stream
        playStream(stream)
      }, {
        // id: 'YOUR_UNIQUE_ID',
        video: true,
        audio: true,
        group: true
      }
    )
    let videos = 
    console.log(document.getElementById('lab').querySelectorAll('video'))
    // streamCanvas();
      
    // Call a peer
    // callPeer(peerId: string)
    // send message to all connected peers
    // sendMessage(message: string)
    // send message to one peer
    // sendMessage(message: string, peerId: string)
  </script>
</body>
</html>